# ${PROGRAM_NAME}

![CI](https://github.com/jeremyeder/${PROGRAM_NAME}/workflows/CI/badge.svg)
![Tests](https://img.shields.io/badge/tests-53%20passing-green.svg)
![Coverage](https://img.shields.io/badge/coverage-comprehensive-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![Quality](https://img.shields.io/badge/code%20quality-production%20ready-green.svg)

`${PROGRAM_NAME}` (LLM Debug) is a mock CLI tool for debugging and simulating `LLMInferenceService` CRDs in a Kubernetes-like environment — without requiring a live cluster. It's designed for SREs, ML engineers, and platform teams to simulate rollouts, test failure modes, and inspect the status of LLM-serving infrastructure offline.

> **Note**: This project was initially generated by ChatGPT but completely rewritten by Claude due to severe structural issues in the original code.

---

## ${EMOJI} Features

- ✅ Offline simulation of up to 100 `LLMInferenceService` CRs
- ✅ Mock support for `LLMInferenceServiceConfig`
- ✅ Full CRUD support on mocked CRs
- ✅ Realistic rollout playback (`play rollout`)
- ✅ Live-style watch command (`watch`) to track CR status changes
- ✅ Log simulation for individual model services
- ✅ Configurable via file structure (`./mocks/`)
- ✅ **${PROGRAM_NAME}top**: Real-time LLM inference monitoring (like `htop` for LLMs)

---

## 📦 Structure

```
${PROGRAM_NAME}/
├── ${PROGRAM_NAME}                 # Python CLI entry point
├── mocks/
│   ├── crs/                   # LLMInferenceService CRs (20 included)
│   ├── config/                # Global LLMInferenceServiceConfig mock
│   ├── pod_logs/              # Simulated logs for LLMs
│   └── topologies/            # Rollout scenarios
```

---

## 🛠 Usage

```bash
chmod +x ${PROGRAM_NAME}
./${PROGRAM_NAME} list                         # List all mocked LLMInferenceServices
./${PROGRAM_NAME} check llm-005                # Inspect a specific CR
./${PROGRAM_NAME} logs llm-005                 # View logs for a model
./${PROGRAM_NAME} config                       # Show global LLM config
./${PROGRAM_NAME} delete llm-005               # Delete a mock CR
./${PROGRAM_NAME} create file.json             # Create CR from file
./${PROGRAM_NAME} simulate canary              # Simulate canary rollout
./${PROGRAM_NAME} ${MONITOR_NAME}                        # Real-time LLM inference monitoring
```

---

## 🔥 ${PROGRAM_NAME}top: Real-time LLM Monitoring

`${PROGRAM_NAME}top` provides real-time monitoring of LLM inference services, similar to `htop` but specifically designed for LLM workloads.

### Features
- **Real-time metrics**: QPS, CPU usage, error rates, latency
- **Multi-model monitoring**: Track multiple LLM services simultaneously  
- **Dual-mode operation**: Works with both mock data and live clusters
- **Interactive display**: Live-updating terminal interface
- **Cluster overview**: Aggregate statistics and health indicators

### Usage

```bash
# Monitor using mock data (default)
./${PROGRAM_NAME} ${MONITOR_NAME}

# Monitor live cluster
./${PROGRAM_NAME} --mode live ${MONITOR_NAME}

# Monitor specific namespace
./${PROGRAM_NAME} ${MONITOR_NAME} --namespace production

# Custom refresh rate
./${PROGRAM_NAME} ${MONITOR_NAME} --interval 1.0

# As kubectl plugin
./${PROGRAM_NAME}top                  # Via symlink
```

### Display

```
${EMOJI} ${PROGRAM_NAME}top - LLM Inference Monitor
Mode: mock | Namespace: default | Runtime: 45s | Sort: qps
Total QPS: 12,847 • Models: 34/34 healthy • Replicas: 127
Avg CPU: 52.3% • Avg Errors: 0.8%
Controls: Ctrl+C to quit | Sort by: qps

┌─── 🔥 Live LLM Inference Traffic ───┐
│ Model                  Status    QPS   CPU   Errors  Latency  Replicas │
│ llama-3-70b-instruct   🟢 Ready   2,847  45%   0.2%    127ms    8       │
│ gpt-4-turbo           🟢 Ready   2,234  62%   0.1%    98ms     6       │
│ claude-3-opus         🟢 Ready   1,923  58%   0.3%    145ms    5       │
│ mixtral-8x7b-instruct 🟢 Ready   1,445  71%   0.5%    156ms    4       │
│ ...                                                                    │
└────────────────────────────────────────────────────────────────────────┘
```

### Requirements

${PROGRAM_NAME}top requires the `rich` library for terminal display:

```bash
pip install rich
```

---

## 🎬 Rollout Simulation

```bash
./${PROGRAM_NAME} simulate canary
./${PROGRAM_NAME} simulate bluegreen
./${PROGRAM_NAME} simulate rolling
./${PROGRAM_NAME} simulate shadow
```

Watch rollouts with animation:

```bash
./watch_rollout.py --topology rolling --autoplay --delay 2
```

---

## 🌐 Live Cluster Support

`${PROGRAM_NAME}` can operate in **live mode** using actual Kubernetes resources.

### 🔁 Modes

| Mode   | Description                       |
|--------|-----------------------------------|
| `mock` | Use local files in `./mocks/`     |
| `live` | Use `kubectl` to talk to a cluster|

### 🔧 Set the Mode

Via environment variable:

```bash
export LLD_MODE=live
./${PROGRAM_NAME} list
```

Or per-command:

```bash
./${PROGRAM_NAME} --mode live list
./${PROGRAM_NAME} --mode mock list
```

---

## 🧪 Development & Testing

**Requires: Python 3.11+**

The tool will check your Python version on startup and exit with an error if you're running an older version.

### Code Quality Features
- ✅ **Comprehensive Testing**: 53 tests including unit and integration tests
- ✅ **Type Hints**: Full type annotations for better IDE support
- ✅ **Security Scanning**: Automated vulnerability scanning with safety and bandit
- ✅ **Code Quality Gates**: Coverage thresholds and complexity checks
- ✅ **Dependency Management**: Automated updates with Dependabot
- ✅ **Error Handling**: Graceful error handling for all operations

### Running Tests
```bash
# Install test dependencies
pip install -r requirements.txt

# Run all tests
python3 -m pytest tests/ -v

# Run with coverage
python3 -m pytest tests/ --cov=. --cov-report=term
```

### Dependencies
See `requirements.txt` for pinned dependency versions with security scanning.

---

## Installation

### 1. Install Python dependencies

**Requirements: Python 3.11 or later**

```bash
# Check your Python version
python3 --version

# Install dependencies
pip install -r requirements.txt
```

### 2. Make executable

```bash
chmod +x ${PROGRAM_NAME}
```

### 3. Test installation

```bash
./${PROGRAM_NAME} help
./${PROGRAM_NAME} list
```

---

## 📜 License

MIT

---

## 🤝 Contributing

Pull requests welcome for:
- Additional rollout topologies
- New simulation features
- Improved error handling
- Enhanced testing